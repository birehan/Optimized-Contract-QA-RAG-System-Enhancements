academy cohort a weekly challenge week llm finetuning enabling quality embedding and text generation for amharic language overview business need aiqem is an african startup focused on ai and blockchain business solutions aiqems motivation is derived from a desire to have a greater impact on ethiopian and african businesses by harnessing technological innovations in the ai and blockchain area aiqems latest agship project released to the ethiopian market is an endtoend ai based telegram ad solution called adbar through a network of telegram bots and extensive data analysis adbar optimally places ads to dierent telegram channels but given telegrams growing prominence as a messaging platform our company needs to adjust its advertising strategy to better t this everchanging ecosystem this project intends to improve the eectiveness of our promotional eorts by integrating powerful ai capabilities for amharic text manipulation in particular creating an amharic rag pipeline that will help generate amharic based creative text ad contents given campaign information such as brief brand and product information and the content history of a telegram channel a successful delivery of this project makes sure that our advertisements are both catchy and relevant to the telegram community to achieve this the technology is required to have quality amharic text embedding and text generation capability in order to do that you should choose a suitable opensource llm model should already have a capability to embed amharic texts such as nous hermes mistral b or amharic language netuned version of llama samuaelllamabtebotamharic or iocuydillamaamharicm and netune it further to deliver the business objective inspirations the following works are our inspiration we envision to collaborate with all stakeholders to create a robust quality llm for the amharic languages llamachinesereadmeenmd at main flagalphallamachinese githubcom data the data contains exported telegram messages in json format representing telegram public channels the channel contains messages of dierent types including service messages and content messages service messages may include details about the creation or modication of the channel while content messages vary in terms of date author and specic content content messages within the channel cover a range of topics featuring updates discussions or multimedia content tailored to the interests of the channels audience these messages may include text in dierent languages images and potentially formatted text elements like bold or italicized sections the structure of the json object provides information about the sender timestamps message types and any associated multimedia content description please nd an example of brief and telegram ad content here you can nd zipped folder data that contains telegram amharic posts from multiple channels here columns in the les in the folder are name the name of the telegram channel type the type of the telegram channel id the unique identier for the telegram channel messages an array containing individual messages within the channel id the unique identier for each message type the type of the message distinguishing between message contentrelated messages and service messages date the timestamp indicating when the message was sent dateunixtime the unix timestamp equivalent of the messages date edited timestamp indicating when the message was last edited editedunixtime the unix timestamp equivalent of the edited date from the sender or actor of the message fromid the unique identier for the sender author the author of the message photo information about any included photo not provided in the json object width the width of the included photo height the height of the included photo text the textual content of the message including an array with amharic text and mention entities textentities information about entities in the text such as mentions with details about the type and text suggested folder structure for your data organisation raw a directory holding all the telegram raw message data parsed a directory holding the telegram parsed message data with columns id telegram channel id text message content date message broadcast datetime label s one or more data labels relevant to your supervised training cleaned a directory holding all the telegram channels cleaned message data nal a directory holding all the message data depending on the need for example this directory might hold the channels word frequencies ngrams and many more data labels ad verticals are categories of businesses that share common characteristics needs and goals in terms of advertising here are some denitions of the ad verticals you asked about retail this vertical includes businesses that sell goods directly to consumers such as clothing groceries furniture books etc retail advertisers aim to drive trac to their physical or online stores increase sales and build brand loyalty automotive this vertical includes businesses that manufacture sell or service vehicles such as cars trucks motorcycles etc automotive advertisers seek to raise awareness generate leads and inuence purchase decisions among potential buyers financial services this vertical includes businesses that provide nancial products or services such as banking insurance investing lending etc financial services advertisers want to educate consumers build trust and encourage conversions or signups telecom this vertical includes businesses that oer telecommunications services such as phone internet cable etc telecom advertisers aim to promote their plans features and benets as well as retain existing customers and acquire new ones cpg consumer products this vertical includes businesses that produce or sell consumer packaged goods such as food beverages household items personal care products etc cpg consumer products advertisers want to increase brand awareness preference and loyalty as well as drive sales and repeat purchases travel this vertical includes businesses that provide travelrelated products or services such as airlines hotels car rentals tours etc travel advertisers want to inspire travelers showcase their destinations and oers and generate bookings and reservations computing products consumer electronics this vertical includes businesses that manufacture or sell computing products or consumer electronics such as laptops smartphones tablets cameras etc computing products consumer electronics advertisers want to demonstrate their products features functionality and value as well as persuade consumers to choose their brand over competitors media this vertical includes businesses that produce or distribute media content such as news entertainment sports etc media advertisers want to attract and engage audiences increase subscriptions or viewership and monetize their content entertainment this vertical includes businesses that provide entertainment products or services such as movies tv shows music video games live events etc entertainment advertisers want to create buzz generate interest and drive consumption or attendance among their target audiences healthcare pharma this vertical includes businesses that provide healthcare or pharmaceutical products or services such as hospitals clinics doctors drugs devices etc healthcare pharma advertisers want to inform consumers patients and professionals as well as promote their solutions and outcomes tags useful to label an add together with one of the above cateogry gambling contains content related to betting or wagering in a realworld or online setting politics political news and media including discussions of social governmental and public policy profanity prominent use of words considered indecent such as curse words and sexual slang pages with only very occasional usage such as news sites that might include such words in a quotation are not included religious targets religion content or promption data labeling group assignment group channel message id range tikvah sheger press all tikvah wasu mohammed all tikvah news all tikvah all tikvah yenetube all tikvah dagu sport all expected outcomes skills experience working with huggingface apis and platform fine tuning and deploying llms experience in using multiple gpus for parallel training and inference working with deep learning frameworks amharic text processing prociency in python programming language prociency in prompt engineering knowledge understanding transformer models and their components understanding the building blocks of instruction based llms understanding chat models such as chatml chat template natural language processing nlp knowledge machine learning and ai knowledgeteam tutors yabebal emitinan rehmet badges each week one user will be awarded one of the badges below for the best performance in the category below in addition to being the badge holder for that badge each badge winner will get points to the overall score visualization quality of visualizations understandability skimmability choice of visualization quality of code reliability maintainability eciency commenting in future this will be cicd cml innovative approach to analysis using latest algorithms adding in research paper content and other innovative approaches writing and presentation clarity of written outputs clarity of slides overall production value most supportive in the community helping others adding links tutoring those struggling the goal of this approach is to support and reward expertise in dierent parts of the machine learning engineering toolbox group work policy this week you are expected to complete the project with your assigned group in the table below your name is assigned to one of the groups we formed group name number group members fill your name base llm model fill it when you choose group group group group group group instructions the rapidly evolving landscape of llms benetted from the unprecedented scales of model size and training data producing models with strong capabilities including reasoning and learning from experience at levels surpassing humans however due to the high imbalance in training data text sources from the internet english dominates in these models models are not as procient in other languages especially lowresource languages that are absent from the multilingual training corpora collecting largescale data for a lowresource language and retraining an llm can be prohibitively expensive due to computational and data collection costs a better approach is transfer learning transferring an llms capabilities from english to a nonenglish language through further pretraining and netuning as part of this challenge you are required to do the following tasks understand the llm landscape as of jan understand the building blocks of llm base models encoder only encoderdecoder decoder only gpu memory needs gb gb gb numbers of gpus and time to full pretraining and netuning how to netune instruct netuning chat netuning understand the key components of llm training and finetuning pretraining selfsupervised learning predicting the next word in a given context supervised ne tuning sft parameterecient tuning peft lowrank adaptation lora overview of best contender open source llm models and their variations mistral b x b llama b falcon b stable ai b openllama b b explore huggingface documentation for inference and netuning test hugging face embedding examples on your local machine tasks test hugging face small llm models on your local machine test hugging face modules for data loading preprocessing batching and tokenizing loading quantized models bitsandbytes applying parameterecient netuining peft applying lora general techniques to reduce memory and netune eciently with a optimal tradeo among memory speed and accuracy understand and prepare amharic data for netuning explore provided data as well as what you can nd in the web prepare data to be ingested in your instructionnetuning pipeline prepare evaluation datasets to benchmark your netuned model with baseline openaihuggingface deployed models select an opensource llm model and netune it build a rag system to generate amharic ad taglines and telegram ad posts given an amharicenglishmixed document brief channel data deploy your rag with simple frontend eg streamlit or react page task literature review huggingface ecosystem in this task you are expected to review basic concepts and methods used to perform further pretraining and netuning of a llm get good understanding of the following concepts and techniques relevant to llms denitions of key terms and concepts hackerllama the llama hitchiking guide to local llms osansevierogithubio background knowledge on llms introduction to large language models understanding llms a comprehensive overview from training to inference transformer architecture encoder decoder selfattention transformers made easy architecture and data ow by mali mnasri opla medium how self attention works in transformer generative ai the science behind large language models simplied llm landscape architecture three categories encoderonly encoderdecoder and decoderonly llm boxing choose your champion comparing the best opensource large language models shakudo library ollamaai embedding of the input data tokenization positional embedding googlesentencepiece unsupervised text tokenizer for neural networkbased text generation githubcom test how tokenization works with openai tokenizer master positional encoding part i by jonathan kernes towards data science understanding positional embeddings in transformer models harrisonpimcom key concepts in finetuning an llm finetuning embedding model with peft and lora hosting a text embedding model that is better cheaper and faster than openais solution get familiar with huggingface ecosystem review huggingface documentation hugging face hub documentation transformers huggingfaceco templates for chat models huggingfaceco try some examples on your local machine building a pdf knowledge bot with opensource llms a stepbystep guide shakudo an introduction to using transformers and hugging face how instruction netuning works huggingfacealignmenthandbook robust recipes for to align language models with human and ai preferences githubcom amharic data collection processing and pipeline for llm netuning llamaamharic llms for low resource languages by garri logistics dec medium task load an llm and use it for inference this task will see you setting up your work environment load an open source pretrained llm and use it to generate output for a variety of scenarios text generation translation question answering summarization etc set up a huggingface account this is required for accessing some open source models eg llama and to upload your netuned model later note about llama to get access to the model you need to a fill this metas form with the same email address you used to create your hugging face account b visit the page of one of the llama available models and accept hugging faces licence terms and acceptable use policy set up your environment gpu enabled notebooks make a choice of open source llm to use a the choice of model including the size of the model depends on the use case and computational resource available i choosing the right opensource llm for your needs ii which open source llm best to finetune b check the comprehensive list of open sourceaccess llms on hugging face c optional you can use huggingchat hugging faces opensource chat ui for llms to check a couple of llms eg mixtralxb and llama model netuned for dialogue load an open source llm from hugging face the size of the model will depend on whether we use quantization a without model quantizationi how to download open source llm models from hugging face and use it locally on your machine ii running a hugging face large language model b with model quantization model quantization with hugging face transformers and bitsandbytes integration inference use the loaded llm to generate output make sure to test multiple inference scenarios text generation translation question answering summarization and test the models ability to handle amharic language a hugging face docs generation with llms b optional use a pipeline for inference task data preprocessing and preparation you may follow llamaamharic llms for low resource languages by garri logistics dec medium to understand the important steps of amharic data preparation for llm netuning suggested tasks be creative to do more are understand the structure of the raw data in the dataraw directory parse the message from the raw json including the features message id channelid date save the parsed data in the dataparsed directory extract and cleanremove features such as remove null values new lines n extra spaces extract and remove hashtags emojis links mentions username other symbols replace with with with with with save the cleaned data in the datacleaned directory task finetuning the llm steps needed to netune the llm steps from inputting the data to model deploymentyou may use garri logistics model iocuydiamharicllamallava githubcom chinesellamaalpaca v released long context llms k and rlhftuned llms facebookresearchllamarecipes examples and recipes for llama model githubcom alignmenthandbookscriptsreadmemd at main huggingfacealignmenthandbook githubcom how to finetune an llm part the huggingface trainer alpacaft weights biases wandbai check out part as well as starter to use the garri logistics model accept llama license on huggingface and download it like this a git lfs install b git clone httpshuggingfacecometallamallamabhf download the amharic netune from huggingface like this a git lfs install b git clone httpshuggingfacecoiocuydillamaamharicm clone httpsgithubcomiocuydiamharicllamallava repository then inside inferenceruninfpy a comment the import safetyutils line b change the mainpath to the path to folder you downloaded from step c change the peftmodel to the path you cloned in the step d go to your llama folder from step and replace all the tokenizer related les with the one you nd from the nd step e set quanitzationtrue inside the main function before the loadmodel function call finally run the inferenceruninfpy le you may choose one of the followng as your base model introducing stable lm b stability ai mistral b mistral ai openweight models pdf arxivorg imoneoiopenchat openchat advancing opensource language models with imperfect data githubcom tiiuaefalconb hugging face llama meta aitask build a rag pipeline to generate telegram amharic ad posts following the techniques you employed in week to build an enterprise grade rag systems for english language apply similar techniques and algorithms to build rag pipeline for amharic language retrive the relevant information from english and amharic texts about advertiser information product being advertised campaign inputs ie briefs ref choosing the right embedding model a guide for llm applications by ryan nguyen medium craft a good prompt generate candidate ads evaluate pipeline tutorials schedule in the following the colour purple indicates morning sessions and blue indicates afternoon sessions monday the mechanics of llms challenge walk through and introduction to transformers introduction to week challenge yabebal overview of llms their transformer architecture and main techniques emtinan key performance indicators understand the project get a good understanding of how llms work tuesday tokenization and vocabulary creation more concepts qa session yabebal tokenization and word embedding natnaelwednesday llms finetuning get a good understanding of data preparation and llm netuning dierent types of netuning a pretrained llm emtinan components of a transformer model yabebal thursday inference llmops more concepts components of a transformer model yabebal advanced use of huggingface rehemet friday deployment more concepts data preparation for instruction tuning natnael deliverables note document should be a pdf stored in google drive or published blog link do not submit a link as pdf if you want to submit pdf document it should be the content of your report not a link interim submission wednesday pm utc link to your code in github repository where you will be using to complete the tasks in this weeks challenge a minimum requirement is that you have a well structured repository and some coding progress is made a review report of your reading and understanding of task and any progress you made in other tasks feedback you may not receive detailed comments on your interim submission but will receive a grade final submission saturday pm utc link to your code in github complete work for finetuning llms with amharic data complete work for generating amharic ad texts complete work for rag quality huggingface deployment frontend a blog post entry which you can submit for example to medium publishing or a pdf report feedback you will receive commentsfeedback in addition to a gradereferences complete beginners guide to hugging face llm tools making llms even more accessible with bitsandbytes bit quantization and qlora peft parameterecient finetuning finetuning large language models llms bert infrastructure benchmarking popular opensource llms llama falcon and mistral truefoundrycom